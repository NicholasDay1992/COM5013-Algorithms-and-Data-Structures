{
 "cells": [
  {
   "attachments": {
    "pythonLogo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![pythonLogo.png](attachment:pythonLogo.png)\n",
    "# Artificial Neural Networks (ANN) #\n",
    "\n",
    "### Content courtesy of OUDCE and IBM, and exercise by Nick Day - September 2022 ###\n",
    "\n",
    "## This section introduces the concept of an Artificial Neural Network (ANN) which continues from the Graph concept. <font color = 'red'> For the 2023-24 run of CO536, there are no further assessed logbook exercises. </font> There is, however, an exercise at the end of this notebook if you would like to implement a basic Artificial Neural Network.\n",
    "\n",
    " - The basis of a Graph class in Python is the implementation of two classes, Graph and Vertex, which establish the essential functionality to create a variety of graphs.\n",
    " - The Vertex class allows for storage of connecting vertices with a dictionary and adjustment of their edges as well.\n",
    " - The Graph class builds upon the Vertex methods and allows addition of vertices and edges, setting the directionality of edges, and determining if a path exists between two vertices.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder on Graphs\n",
    "\n",
    "![graph_weighted](https://dist.neo4j.com/wp-content/uploads/20181010052926/dijkstras-algorithm-example-part-one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "An Artificial Neural Network (ANN) is based on a biological neural network. In order to understand ANNs, it will help to quickly revisit the biological structure of a neuron. The human brain is regarded to be a humungous biological neural network, and an ANN aims to closely simulate this in order to simulate brain capacity in machines. \n",
    "\n",
    "![biologicalnet](https://miro.medium.com/max/610/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
    "\n",
    "\"How a biological neuron is modelled as a mathematical function. In the above panel, we see an illustration of a biological neuron, where its dendrites receive electrical signals in from nerves and other neurons, the charge builds up in the cell-body that then fires an electric charge out of its axon after reaching some threshold of internal charge. This is modelled mathematically, as we see in the lower panel, where a number of input values, which are analogous to electrical charge incoming on biological dendrites, are summed and compared to a threshold to determine the output value returned by the function. The output is analogous to the charge outgoing on the biological axon.\" \n",
    "<b>Derived from Applied Go, Perceptrons- the most basic form of a neural network article, June 09, 2016</b>\n",
    "\n",
    "\n",
    "Artificial neurons in a network seek to mimic certain parts of biological neurons, such as dendrites, cell bodies, and axons, using simplified mathematical models of what limited knowledge we have on their inner workings. Signals can be received from dendrites and sent down the axon once enough signals are received. This outgoing signal can then be used as another input for other neurons, repeating the process. Some signals are more important than others and can trigger some neurons to fire easier. Connections can become stronger or weaker; new connections can appear while others can cease to exist. \n",
    "\n",
    "Whilst there is still much more to understand about how the brain works, we can still try to mimic most of this process by coming up with a function that receives a list of weighted input signals and outputs some signal if the sum of these weighted inputs reaches a certain bias. Although this mathematical neuron is nowhere near as sophisticated as a biological one, it's still powerful enough to work with simpler tasks like classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neuron \n",
    "\n",
    "![AN](https://miro.medium.com/max/777/1*YA-0MyBLsn8BiThBGjycIA.png)\n",
    "\n",
    "The mathematical functions an artificial neuron performs. The inputs are represented by the x values. The weights on the inputs are represented by the w values. Each x is multiplied by its corresponding w to apply the weight to the input value. This is then summed (represented by the epsilon) and added to a bias constant, b. The total input is then passed to the activation function that determines the artificial neuron's output value, y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function\n",
    "\n",
    "The sigmoid function is used as an activation function in a neural network. A weighted sum of inputs is passed through this function to normalise outputs, either between 0 and 1 or -1 and +1 before the next layer of the network.\n",
    "\n",
    "![sigmoid](https://raw.githubusercontent.com/Codecademy/docs/main/media/sigmoid-function.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Activation Function\n",
    "![Linear](https://miro.medium.com/v2/resize:fit:1400/1*tldIgyDQWqm-sMwP7m3Bww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (Rectified Linear Unit)\n",
    "\n",
    "The sigmoid and hyperbolic tangent activation functions cannot be used in networks with many layers due to the vanishing gradient problem.\n",
    "Here's an example with the sigmoid activation function:\n",
    "![sigmoid_vanishing](https://miro.medium.com/v2/resize:fit:2000/1*6A3A_rt4YmumHusvTvVTxw.png).\n",
    "\n",
    "The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.\n",
    "\n",
    "![Linear_RELU_](Sigmoid_linear_relu.png)\n",
    "\n",
    "[More on ReLU here](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\n",
    "\n",
    "![]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other activation functions \n",
    "\n",
    "![other_activation](https://miro.medium.com/v2/resize:fit:444/1*BrcwfzrjHoHf3jfSfmE5ww.png)\n",
    "\n",
    "![activation_functions](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Vwhp6Ia8BDT_fdGN7iJlUg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big O notation measures growth of an algorithm (block of code which performs a function)\n",
    "\n",
    "The notation is written as O ( f(n) ) where f(n) represents the growth rate of the algorithm in terms of the input size n\n",
    "\n",
    "![bigo](https://cdn-media-1.freecodecamp.org/images/1*KfZYFUT2OKfjekJlCeYvuQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "![ANN_hiddenlayer](https://miro.medium.com/max/1200/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png)\n",
    "\n",
    "Multiple artificial neurons can be connected to form an ANN. In this illustration, neurons are as circles and the connections as arrows from left to right. These connections link the output of one neuron to the inputs of others. Here, we first have 3 neurons (the input layer) that feed into a layer of 4 neurons (hidden layer 1), which feed into another layer of 4 (hidden layer 2) and finally a single neuron as the output layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network example\n",
    "\n",
    "![weightAndHeight](https://victorzhou.com/27cf280166d7159c0465a58c68f99b39/network3.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "# https://victorzhou.com/blog/intro-to-neural-networks/ \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n",
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "# https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ... code from previous section here\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994\n",
    "\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    # The Neuron class here is from the previous section\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    # The inputs for o1 are the outputs from h1 and h2\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.276\n",
      "Epoch 10 loss: 0.073\n",
      "Epoch 20 loss: 0.057\n",
      "Epoch 30 loss: 0.046\n",
      "Epoch 40 loss: 0.038\n",
      "Epoch 50 loss: 0.032\n",
      "Epoch 60 loss: 0.027\n",
      "Epoch 70 loss: 0.024\n",
      "Epoch 80 loss: 0.021\n",
      "Epoch 90 loss: 0.018\n",
      "Epoch 100 loss: 0.017\n",
      "Epoch 110 loss: 0.015\n",
      "Epoch 120 loss: 0.014\n",
      "Epoch 130 loss: 0.013\n",
      "Epoch 140 loss: 0.012\n",
      "Epoch 150 loss: 0.011\n",
      "Epoch 160 loss: 0.010\n",
      "Epoch 170 loss: 0.010\n",
      "Epoch 180 loss: 0.009\n",
      "Epoch 190 loss: 0.008\n",
      "Epoch 200 loss: 0.008\n",
      "Epoch 210 loss: 0.008\n",
      "Epoch 220 loss: 0.007\n",
      "Epoch 230 loss: 0.007\n",
      "Epoch 240 loss: 0.007\n",
      "Epoch 250 loss: 0.006\n",
      "Epoch 260 loss: 0.006\n",
      "Epoch 270 loss: 0.006\n",
      "Epoch 280 loss: 0.006\n",
      "Epoch 290 loss: 0.005\n",
      "Epoch 300 loss: 0.005\n",
      "Epoch 310 loss: 0.005\n",
      "Epoch 320 loss: 0.005\n",
      "Epoch 330 loss: 0.005\n",
      "Epoch 340 loss: 0.005\n",
      "Epoch 350 loss: 0.004\n",
      "Epoch 360 loss: 0.004\n",
      "Epoch 370 loss: 0.004\n",
      "Epoch 380 loss: 0.004\n",
      "Epoch 390 loss: 0.004\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 410 loss: 0.004\n",
      "Epoch 420 loss: 0.004\n",
      "Epoch 430 loss: 0.004\n",
      "Epoch 440 loss: 0.003\n",
      "Epoch 450 loss: 0.003\n",
      "Epoch 460 loss: 0.003\n",
      "Epoch 470 loss: 0.003\n",
      "Epoch 480 loss: 0.003\n",
      "Epoch 490 loss: 0.003\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 510 loss: 0.003\n",
      "Epoch 520 loss: 0.003\n",
      "Epoch 530 loss: 0.003\n",
      "Epoch 540 loss: 0.003\n",
      "Epoch 550 loss: 0.003\n",
      "Epoch 560 loss: 0.003\n",
      "Epoch 570 loss: 0.003\n",
      "Epoch 580 loss: 0.003\n",
      "Epoch 590 loss: 0.003\n",
      "Epoch 600 loss: 0.002\n",
      "Epoch 610 loss: 0.002\n",
      "Epoch 620 loss: 0.002\n",
      "Epoch 630 loss: 0.002\n",
      "Epoch 640 loss: 0.002\n",
      "Epoch 650 loss: 0.002\n",
      "Epoch 660 loss: 0.002\n",
      "Epoch 670 loss: 0.002\n",
      "Epoch 680 loss: 0.002\n",
      "Epoch 690 loss: 0.002\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 710 loss: 0.002\n",
      "Epoch 720 loss: 0.002\n",
      "Epoch 730 loss: 0.002\n",
      "Epoch 740 loss: 0.002\n",
      "Epoch 750 loss: 0.002\n",
      "Epoch 760 loss: 0.002\n",
      "Epoch 770 loss: 0.002\n",
      "Epoch 780 loss: 0.002\n",
      "Epoch 790 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 810 loss: 0.002\n",
      "Epoch 820 loss: 0.002\n",
      "Epoch 830 loss: 0.002\n",
      "Epoch 840 loss: 0.002\n",
      "Epoch 850 loss: 0.002\n",
      "Epoch 860 loss: 0.002\n",
      "Epoch 870 loss: 0.002\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 960 loss: 0.001\n",
      "Epoch 970 loss: 0.001\n",
      "Epoch 980 loss: 0.001\n",
      "Epoch 990 loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"red\">Exercise</font> ##\n",
    "\n",
    "Insert a 'code' cell below. In this do the following:\n",
    "\n",
    "- 1 - Run the first neural network example above.\n",
    "- 2 - Can you re-write the first neural network example to use a graph? How would you adapt the inputs, the sigmoid functions and the outputs?\n",
    "- 3 - Run the second ANN example (with the training algorithm) - what do you notice?\n",
    "- 4 - What is the difference between a supervised and unsupervised learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References & Learning Resources on ANNs\n",
    "\n",
    " - ANN Resource: https://www.ibm.com/cloud/learn/neural-networks\n",
    "\n",
    " - Towards DataScience: https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6\n",
    "\n",
    " - Victor Zhao: https://victorzhou.com/blog/intro-to-neural-networks/\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c24ecc2928a1df641fb89905a028e959d05eac4f79c1af53f0bb766b01409011"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
